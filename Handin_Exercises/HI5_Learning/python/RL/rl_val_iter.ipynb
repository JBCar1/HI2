{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in Exercise 5: Learning for autonomous vehicles --- Reinforcement learning and value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration for solving an extension of Example 6.6 in\n",
    "Sutton, R. S., & A. G. Barto: Reinforcement learning: An introduction.\n",
    "MIT Press, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from grid_world import plot_value_and_policy, p_grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you want plots in external windows\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the parameters for the Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99   # Discount factor\n",
    "R_goal = 0.0   # Reward for reaching goal state\n",
    "R_sink = -10.0   # Reward for reaching 'cliff' states\n",
    "R_grid = -0.1   # Reward for remaining states\n",
    "\n",
    "P_move_action = 0.99 # Probability of moving in the direction specified by action\n",
    "P_dist = (1 - P_move_action) / 2  # Probability of moving sideways compared to \n",
    "                                  # intended because of disturbance\n",
    "\n",
    "# Define size of grid world, goal, and cliff states\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 5\n",
    "\n",
    "goal = (3, 4)   # Element index goal state\n",
    "sink = [(3, 1), (3, 2), (3, 3)]   # Element indices for cliff states\n",
    "\n",
    "# Setup reward matrix R\n",
    "\n",
    "R = np.full((n_rows, n_cols), fill_value=R_grid)\n",
    "R[goal[0], goal[1]] = R_goal\n",
    "for p in sink:\n",
    "    R[p[0], p[1]] = R_sink\n",
    "\n",
    "# Occupancy grid defines states where there are obstacles (0 - no\n",
    "# obstacles, 1 - obstacles)\n",
    "\n",
    "occ_grid = np.zeros((n_rows, n_cols), dtype=int)\n",
    "occ_grid[1, 1] = 1\n",
    "\n",
    "# Save all parameters in a dictionary\n",
    "params = {'gamma': gamma, 'R_goal': R_goal, 'R_sink': R_sink,\n",
    "          'R_grid': R_grid, 'P_move_action': P_move_action,\n",
    "          'P_dist': P_dist, 'n_rows': n_rows, 'n_cols': n_cols,\n",
    "          'goal': goal, 'sink': sink, 'R': R, 'occ_grid': occ_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop for value iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press return to proceed to next iteration. Note that plots must be made in an external window, not inline, since value function and policy are updated in the `plot_iter` function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for value function V and policy Pi for each state s\n",
    "V = np.zeros((n_rows, n_cols))\n",
    "# Actions - ['left', 'right', 'up', 'down'] counted as 0-3\n",
    "Pi = np.full((n_rows, n_cols), fill_value=-1)\n",
    "\n",
    "converged = False\n",
    "num_iterations = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm according to Section 4.4 in Sutton, R. S., & A. G. Barto: \n",
    "Reinforcement learning: An introduction. MIT Press, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the iterations until convergence criterion is fulfilled\n",
    "while not converged:\n",
    "    Delta = 0\n",
    "    # Go through all states\n",
    "    for s in product(range(n_rows), range(n_cols)):\n",
    "        row, col = s\n",
    "        # Check if obstacle or sink states, then continue to next state\n",
    "        if ((occ_grid[row, col] == 1) or s == goal or s in sink):\n",
    "            continue\n",
    "        v = V[row, col]\n",
    "        V_a = np.zeros(4)\n",
    "        for a in range(4):  # Go through all possible actions\n",
    "            for s_prim, r, p_prim in p_grid_world(s, a, params):  \n",
    "                # Three state transitions are possible in \n",
    "                # each state, sum the terms for each of them\n",
    "                V_prim = V[s_prim[0], s_prim[1]]\n",
    "                V_a[a] += p_prim * (r + params['gamma'] * V_prim)\n",
    "                \n",
    "        # Compute new value function V for current state and update\n",
    "        # policy Pi\n",
    "        max_a = np.argmax(V_a)\n",
    "        Pi[row, col] = max_a\n",
    "        V[row, col] = V_a[max_a]\n",
    "\n",
    "        # Compute improvement criterion for V\n",
    "        Delta = np.max((Delta, np.abs(v - V[row, col])))\n",
    "\n",
    "    # Visualize the current value function V and policy Pi in the grid\n",
    "    # world\n",
    "    plot_value_and_policy(V, Pi, params)\n",
    "\n",
    "    num_iterations += 1\n",
    "\n",
    "    # Display current value function V and policy Pi\n",
    "    print(V)\n",
    "    print(Pi)\n",
    "    print('Press enter')\n",
    "    _ = input()\n",
    "\n",
    "    # If improvement criterion below threshold, stop the value iteration\n",
    "    # since convergence has been achieved\n",
    "    if Delta < 1e-6:\n",
    "        converged = True\n",
    "print(f'Convergence after {num_iterations} iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
