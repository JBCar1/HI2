{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in Exercise 5: Learning for autonomous vehicles --- Reinforcement learning and Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning for solving an extension of Example 6.6 in\n",
    "Sutton, R. S., & A. G. Barto: Reinforcement learning: An introduction.\n",
    "MIT Press, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from seaborn import despine\n",
    "from grid_world import plot_value_and_policy, p_grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Run if you want plots in external windows\n",
    "# %matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the parameters for the Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99   # Discount factor\n",
    "R_goal = 0.0   # Reward for reaching goal state\n",
    "R_sink = -10.0   # Reward for reaching 'cliff' states\n",
    "R_grid = -0.1   # Reward for remaining states\n",
    "\n",
    "alpha = 0.5   # Learning rate in Q-update\n",
    "eps = 0.5   # Epsilon-greedy parameter\n",
    "\n",
    "P_move_action = 1.0  # Probability of moving in the direction specified by action\n",
    "P_dist = (1 - P_move_action) / 2  # Probability of moving sideways compared to \n",
    "                                  # intended because of disturbance\n",
    "\n",
    "# Define size of grid world, goal, and cliff states\n",
    "n_rows = 4\n",
    "n_cols = 5\n",
    "\n",
    "goal = (3, 4)  # Element index goal state\n",
    "sink = [(3, 1), (3, 2), (3, 3)]  # Element indices for cliff states\n",
    "\n",
    "# Setup reward matrix R\n",
    "R = np.full((n_rows, n_cols), fill_value=R_grid)\n",
    "R[goal[0], goal[1]] = R_goal\n",
    "for p in sink:\n",
    "    R[p[0], p[1]] = R_sink\n",
    "\n",
    "# Occupancy grid defines states where there are obstacles (0 - no\n",
    "# obstacles, 1 - obstacles)\n",
    "occ_grid = np.zeros((n_rows, n_cols))\n",
    "occ_grid[1, 1] = 1\n",
    "\n",
    "# Save parameters in a dictionary\n",
    "params = {'gamma': gamma, 'R_goal': R_goal, 'R_sink': R_sink,\n",
    "          'alpha': alpha, 'eps': eps,\n",
    "          'R_grid': R_grid, 'P_move_action': P_move_action, \n",
    "          'P_dist': P_dist, 'n_rows': n_rows, 'n_cols': n_cols, \n",
    "          'goal': goal, 'sink': sink, 'R': R, 'occ_grid': occ_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary function used in the main loop for Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = np.random.default_rng(seed=1891)  # Random number generator used when simulating actions\n",
    "\n",
    "# Function for computing the next state s_prim and its associated reward\n",
    "def next_state(s, a, params):\n",
    "    p_next_state = p_grid_world(s, a, params)\n",
    "    s_prim, r, _ = p_next_state[rg.choice(range(len(p_next_state)), p=[p_i[2] for p_i in p_next_state])]\n",
    "    return s_prim, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-greedy exploration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_eps_greedy(s_curr, itr_nbr, Q, params):\n",
    "    \"\"\"Select the action to take at a particular state s_curr using \n",
    "      an epsilon-greedy strategy\n",
    "    \n",
    "      action = select_eps_greedy(s_curr, k, Q, params):\n",
    "      \n",
    "      Input:\n",
    "          s_curr - current state\n",
    "          itr_nbr - current iteration number\n",
    "          Q - Q matrix\n",
    "          params - parameter dictionary\n",
    "          \n",
    "      Output:\n",
    "          action - selected action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constant epsilon over episodes\n",
    "    eps = params['eps']\n",
    "\n",
    "    # Sample a number between 0 and 1, select action based on the outcome\n",
    "    rnd = np.random.uniform()\n",
    "    \n",
    "    # Find the best action according to current Q at state s_curr\n",
    "    max_a = np.argmax(Q[s_curr[0], s_curr[1]])\n",
    "\n",
    "    # Create vector with remaining actions\n",
    "    a_list = []\n",
    "    for a in range(4):\n",
    "        if not a == max_a:\n",
    "            a_list.append(a)\n",
    "\n",
    "    # Select action according to sampled random value\n",
    "    if rnd < 1 - eps + eps / 4:\n",
    "        action = max_a\n",
    "    elif rnd < 1 - eps + eps / 2:\n",
    "        action = a_list[0]\n",
    "    elif rnd < 1 - eps + 3 * eps / 4:\n",
    "        action = a_list[1]\n",
    "    else:\n",
    "        action = a_list[2]\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop for Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variable for value function V for each state s\n",
    "V = np.zeros((n_rows, n_cols))\n",
    "\n",
    "# Initialize object for Q-function with random values (function of\n",
    "# state s and action a)\n",
    "Q = np.random.uniform(size=(n_rows, n_cols, 4))  # Number of rows x number of columns x number \n",
    "                                                 # of actions\n",
    "\n",
    "# Initialize Q for terminal states to zero\n",
    "Q[goal[0], goal[1]] = 0.0\n",
    "for si in sink:\n",
    "    Q[si[0], si[1]] = 0.0\n",
    "    \n",
    "# Initialize vector for policy Pi\n",
    "# Actions - ['left', 'right', 'up', 'down'] counted as 0-3\n",
    "Pi = np.full((n_rows, n_cols), fill_value=-1)\n",
    "\n",
    "# Define number of iterations for Q-learning\n",
    "nbr_iters = 2000\n",
    "\n",
    "# Initialize vector for sum of rewards for each episode\n",
    "sum_r = np.zeros(nbr_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm according to Section 6.5 in Sutton, R. S., & A. G. Barto: \n",
    "Reinforcement learning: An introduction. MIT Press, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = False\n",
    "# Run nbr_iters episodes of Q-learning\n",
    "for k in range(nbr_iters):\n",
    "    # Start state\n",
    "    s_curr = (n_rows - 1, 0)\n",
    "    \n",
    "    terminal_state = False\n",
    "    \n",
    "    # Continue Q-learning episode until terminal state reached\n",
    "    while not terminal_state:\n",
    "        # Select action according to epsilon-greedy strategy\n",
    "        action = select_eps_greedy(s_curr, k, Q, params)\n",
    "        \n",
    "        # Perform the action and receive reward and next state s_prim\n",
    "        s_prim, r = next_state(s_curr, action, params)\n",
    "\n",
    "        # Q-learning update of action-value function\n",
    "        Q[s_curr[0], s_curr[1], action] = (\n",
    "            Q[s_curr[0], s_curr[1], action] + \n",
    "            alpha * (r + gamma * np.max(Q[s_prim[0], s_prim[1]]) - \n",
    "            Q[s_curr[0], s_curr[1], action]))\n",
    "        \n",
    "        # Update the sum of reward vector\n",
    "        sum_r[k] = sum_r[k] + r\n",
    "        \n",
    "        # Move to next state\n",
    "        s_curr = s_prim\n",
    "        \n",
    "        # Check if a terminal state has been reached (goal or sink states, \n",
    "        # closes an episode)\n",
    "        if s_curr == goal or s_curr in sink:\n",
    "            terminal_state = True\n",
    "            \n",
    "            # Update value function V and policy Pi\n",
    "            for row in range(n_rows):\n",
    "                for col in range(n_cols):\n",
    "                    if (occ_grid[row, col] == 1) or (row, col) == goal or (row, col) in sink:\n",
    "                        continue\n",
    "                    # Compute value function V at current state from Q\n",
    "                    # function\n",
    "                    max_a = np.argmax(Q[row, col])\n",
    "                    V_ij = Q[row, col, max_a]\n",
    "                    V[row, col] = V_ij\n",
    "                    # Update policy Pi with the currently best action at\n",
    "                    # current state (according to Q function)\n",
    "                    Pi[row, col] = max_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the value function and policy after all iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_and_policy(V, Pi, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average of reward for N episodes for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "mean_sum_r = np.zeros(sum_r.shape[0])\n",
    "\n",
    "for k in range(N, sum_r.shape[0]):\n",
    "    mean_sum_r[k] = np.mean(sum_r[k - N:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the evolution of the reward for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, clear=True)\n",
    "plt.plot(mean_sum_r[N:], lw=0.5)\n",
    "plt.title(f'Sum of rewards for each episode (average over {N})')\n",
    "despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
